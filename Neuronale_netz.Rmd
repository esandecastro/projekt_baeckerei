---
title: "Estimation of Neural Net"
output: html_notebook
---

### # Installation ggf. noch benötigter Pakete ###
```{r}
# Nur ausführen, beim allerersten Mal !!
#install.packages("fastDummies")
#install.packages("reticulate")
#install.packages("Metrics")
#library(reticulate)
#py_install("numpy")
#py_install("tensorflow")
```


### Vorbereitung der Umgebung ###
```{r}
# Umgebungsvariablen löschen
remove(list = ls())
# Einbinden benötogter Funktionsbibliotheken
library(reticulate)
library(readr)
library(fastDummies)
library(ggplot2)
library(Metrics)
# Installieren der benötigten Pakete
if (!require(rstudioapi)) {
  install.packages("rstudioapi")
  library(rstudioapi) 
}
if (!require(broom)) {
  install.packages("broom")
  library(broom)
}
# Arbeitsspeicher leeren
remove(list = ls())
graphics.off()
# Funktionsdefinitionen
#' Title Fast creation of normalized variables
#' Quickly create normalized columns from numeric type columns in the inputted data. This function is useful for statistical analysis when you want normalized columns rather than the actual columns.
#'
#' @param .data An object with the data set you want to make normalized columns from.
#' @param norm_values Dataframe of column names, means, and standard deviations that is used to create corresponding normalized variables from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) with same number of rows as inputted data and original columns plus the newly created normalized. columns.
#' @export
#'
#' @examples
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data$norm <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
    names(.data)[length(.data)] <- paste0(norm_values$name[i], "_norm")
  }
  return (.data)
}
#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
#'
#' @param .data A data set including the variables you want to get the means and standard deviations from.
#' @param select_columns A vector with a list of variable names for which you want to get the means and standard deviations from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) including the names, means, and standard deviations of the variables included in the select_columns argument.
#' @export
#'
#' @examples
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]], na.rm=TRUE)
    sd <- sd(.data[[col_name]], na.rm=TRUE)
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}
```


### Aufbereitung der Daten ###
```{r}
# Einlesen der Daten
#house_pricing <- read_csv("https://raw.githubusercontent.com/opencampus-sh/ws1920-datascience/master/house_pricing_test.csv")
# Arbeitsverzeichnis auf das Projektverzeichnis setzen
proj_pfad <- getActiveProject()
setwd(proj_pfad)

# Durchführen des Aufbereitungs-Skripts ("source" führt zum fehlerhaften Durchführen mit Umlauten, daher die explizite Angabe der Unicode-Kodierung)
eval(parse("Aufbereitung.R", encoding = "UTF-8"))

```

```{r}
# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
#dummy_list <- c("view", "waterfront")
dummy_list <- c("Ferien", "FerienCAU")
#house_pricing_dummy = dummy_cols(house_pricing, dummy_list)
daten_dummy = dummy_cols(daten, dummy_list)
# Standardisierung von metrischen Variablen
norm_list <- c("Umsatz", "Temperatur", "Windgeschwindigkeit", "Bewoelkung", "Wettercode", "Umsatz_letzte_1_Tage", "Umsatz_letzte_3_Tage", "Umsatz_letzte_7_Tage", "Umsatz_letzte_30_Tage")

#Bewoelkung", "Temperatur", "Windgeschwindigkeit", "Umsatz_letze_1__Tage", "Umsatz_letze_2__Tage", "Umsatz_letze_3__Tage", "Umsatz_letze_7__Tage", "Umsatz_letze_30__Tage"
# Berechnung der Mittelwerte und Standardabweichungen der zu standardisierenden Variablen
norm_values_list <- get.norm_values(daten_dummy, norm_list)
# Standardisierung der angegebenen metrischen Variablen
daten_norm <- norm_cols(daten_dummy, norm_values_list)
# Definition von Variablenlisten, um das Arbeiten mit diesen zu erleichtern
ferien_dummies = c('Ferien_0', 'Ferien_1')
ferienCAU_dummies = c('FerienCAU_0', 'FerienCAU_1')
# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)
#"Umsatz_letze_1__Tage_norm", "Umsatz_letze_3__Tage_norm", "Umsatz_letze_7_Tage_norm", "Umsatz_letze_30__Tage_norm"
features = c('Temperatur_norm', "Windgeschwindigkeit_norm", "Bewoelkung_norm", "Wettercode_norm", "Umsatz_letzte_1_Tage_norm", "Umsatz_letzte_3_Tage_norm", "Umsatz_letzte_7_Tage_norm", "Umsatz_letzte_30_Tage_norm", ferien_dummies, ferienCAU_dummies)
# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = 'Umsatz_norm'
# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(1)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(daten_norm)), size = floor(0.80 * nrow(daten_norm)))
# Teilen in Trainings- und Testdatensatz
train_dataset = daten_norm[train_ind, features]
test_dataset = daten_norm[-train_ind, features]
# Selektion der Variable, die als Label definiert wurde
train_labels = daten_norm[train_ind, label]
test_labels = daten_norm[-train_ind, label]
```


### Schätzung des Neuronalen Netzes
```{python}
# Benoetigte Python Libraries einbinden
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# Definition der Form des tiefen neuronalen Netzes (Deep Neural Nets)
model = keras.Sequential([
  layers.Dense(5, activation='relu', input_shape=[len(r.train_dataset.keys())]),
  layers.Dense(4, activation='relu'),
  layers.Dense(1)
])
# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse",
              optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))
# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()
# Schaetzung des Modells
history = model.fit(r.train_dataset, r.train_labels, epochs=150, validation_split = 0.1, verbose=0)
```


### Speichern des Neuronalen Netzes für spätere Vorhersagen ###
```{python}
model.save("neuronal_model.h5")
```


### Auswertung der Modelloptimierung ###
```{r}
# Grafische Ausgabe der Modelloptimierung
# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))
# Plot
ggplot(data[-1,]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 
```


### Laden eines gespeicherten Neuronalen Netzes ###
```{python}
model1 = keras.models.load_model("neuronal_model_2.h5")
#model2 = keras.models.load_model("neuronal_model_2.h5")
#model3 = keras.models.load_model("neuronal_model_3.h5")
#model4 = keras.models.load_model("neuronal_model_4.h5")

```


### Auswertung der Schätzergebnisse ###
```{r}

models <- list(py$model1)

for (j in 1:length(models)) {
  model <- models[[j]]
  
  # Schätzung der (normierten) Preise für die Trainings- und Testdaten
train_predictions_norm <- model$predict(train_dataset)
test_predictions_norm <- model$predict(test_dataset)
# Rückberechnung der normierten Preisschätzungen zu den tatsächlichen Preisschätzungen bzw. Preisen
train_predictions <- (train_predictions_norm * norm_values_list$sd[1] ) + norm_values_list$mean[1]
test_predictions <- (test_predictions_norm * norm_values_list$sd[1]) + norm_values_list$mean[1]
# Selektion der zugehörigen tatsächlichen Preise
train_actuals <- daten$Umsatz[train_ind]
test_actuals <- daten$Umsatz[-train_ind]
# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals, train_predictions)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Test Data:\t\t", format(mape(test_actuals, test_predictions)*100, digits=3, nsmall=2)))
}

```

```{r}
## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten
# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = train_predictions/1000, actual = train_actuals/1000)
data_test <- data.frame(prediction = test_predictions/1000, actual = test_actuals/1000)
# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 
# Plot der Ergebnisse der Testdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 
```

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorergesagter Umsatz:\t", format(test_predictions[100], digits=2, nsmall =0)))
cat(paste0("\nTatsächlicher Umsatz:\t", test_actuals[100]))
```